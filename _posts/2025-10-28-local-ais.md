---
layout: post
title: "Local AIs with Ollama"
date: 2025-10-28
categories: ["Artificial Intelligence", "Local Development"]
image: /assets/img/ollama.png
---

Local AI models are revolutionizing how we interact with artificial intelligence. This devnote explores Ollama, a powerful tool for running AI models locally on your own PC. I'll cover everything you need to know about getting started with local AI development.

1. [Why Local AI, Though?](#why-local-ai-though)
2. [Quick Setup](#quick-setup)
    1. [Install Ollama](#install-ollama)
    2. [Basic Usage](#basic-usage)
    3. [API Integration](#api-integration)
3. [(some of my) Favorite Models!](#some-of-my-favorite-models)
4. [Performance Notes](#performance-notes)
5. [Where would this be used?](#where-would-this-be-used)
6. [Resources :)](#resources)
7. [Personal Tips](#personal-tips)
{:toc}

## Why Local AI, Though?
I just need four points to answer this:
- **Privacy at most** (data stays on your machine, not on the ‚òÅ)
- **No more internet dependency**
- **No usage costs**
- **Full control over model behavior**

## Quick Setup
I made this guide to be as simple as possible (for you beginners), so we'll use Ollama, a simple CLI tool for using local AIs.
### Install Ollama
~~~bash
# Mac
curl https://ollama.ai/install.sh | sh

# Linux
curl -fsSL https://ollama.ai/install.sh | sh
~~~
For Windows, go to https://ollama.com/download/windows and follow the instructions.
### Basic Usage
~~~bash
# Pull a model
ollama pull llama2

# Basic interaction
ollama run llama2
# or, if you want to use a specific parameter size model
ollama run llama2:13b
~~~
Honestly, skipping the pull command is possible, but it needs an extra step (which is downloading) before it can run the model.

### API Integration

~~~bash
# Simple API call
curl -X POST http://localhost:11434/api/generate -d '{
    "model": "llama2",
    "prompt": "Why is local AI important?"
}'
~~~

## (some of my) Favorite Models!
- **llama2**: good all-rounder
- **codellama**: programming focused
- **mistral**: great performance/size ratio
If you want to go ahead and run some of the best AI models:
- **gpt4all**: tiny, experimental, great for offline demos (very low resource)  
- **vicuna**: chat-focused, tuned for conversational assistants (mid-weight)  
- **falcon (7B variant)**: high-quality, efficient 7B model (performance-oriented)  

If you also have higher resources (more RAM / GPUs), consider larger variants:
- **llama2 (13B / 70B variants)**: scalable chat and instruction tuning; pick 13B for a balance, 70B for top quality  
- **falcon (40b variant)**: strong 40B option for high-quality generation (heavy)  
- **Mistral / other large instruction-tuned models (e.g., large Mistral variants)**: great tradeoffs depending on release/version. use `mistral:large`

## Performance Notes
- RAM usage varies by model
- 16GB RAM minimum recommended
- GPU acceleration helps significantly

## Where would this be used?
Well, it would be useful for cutting down costs significantly, and also for more privacy. Big companies take your chats and (probably) sell them. You don't want that, do you? Also, you don't need to invest in much money for AI models like GPT-5 (if you use it more than usual).

## Resources :)
- [Ollama GitHub](https://github.com/ollama/ollama)
- [Model Library](https://ollama.ai/library)

## Personal Tips
- Keep smaller models for quick tasks
- Use modelfiles for customization
- Watch RAM usage with larger models

These are notes from my experience with local AI models. Your mileage may vary based on hardware and specific use cases.
{:.note}