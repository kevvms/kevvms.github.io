<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-10-29T20:11:40+07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">kev’s devnotes</title><subtitle>code notes, experiments, and snippets.. you name it.</subtitle><author><name>kev</name><email>kevinm.s@outlook.co.id</email></author><entry><title type="html">Local AIs with Ollama</title><link href="http://localhost:4000/artificial%20intelligence/local%20development/2025/10/28/local-ais.html" rel="alternate" type="text/html" title="Local AIs with Ollama" /><published>2025-10-28T00:00:00+07:00</published><updated>2025-10-28T00:00:00+07:00</updated><id>http://localhost:4000/artificial%20intelligence/local%20development/2025/10/28/local-ais</id><content type="html" xml:base="http://localhost:4000/artificial%20intelligence/local%20development/2025/10/28/local-ais.html"><![CDATA[<p>Local AI models are revolutionizing how we interact with artificial intelligence. This devnote explores Ollama, a powerful tool for running AI models locally on your own PC. I’ll cover everything you need to know about getting started with local AI development.</p>

<ol id="markdown-toc">
  <li><a href="#why-local-ai-though" id="markdown-toc-why-local-ai-though">Why Local AI, Though?</a></li>
  <li><a href="#quick-setup" id="markdown-toc-quick-setup">Quick Setup</a>    <ol>
      <li><a href="#install-ollama" id="markdown-toc-install-ollama">Install Ollama</a></li>
      <li><a href="#basic-usage" id="markdown-toc-basic-usage">Basic Usage</a></li>
      <li><a href="#api-integration" id="markdown-toc-api-integration">API Integration</a></li>
    </ol>
  </li>
  <li><a href="#some-of-my-favorite-models" id="markdown-toc-some-of-my-favorite-models">(some of my) Favorite Models!</a></li>
  <li><a href="#performance-notes" id="markdown-toc-performance-notes">Performance Notes</a></li>
  <li><a href="#where-would-this-be-used" id="markdown-toc-where-would-this-be-used">Where would this be used?</a></li>
  <li><a href="#resources-" id="markdown-toc-resources-">Resources :)</a></li>
  <li><a href="#personal-tips" id="markdown-toc-personal-tips">Personal Tips</a></li>
</ol>

<h2 id="why-local-ai-though">Why Local AI, Though?</h2>
<p>I just need four points to answer this:</p>
<ul>
  <li><strong>Privacy at most</strong> (data stays on your machine, not on the ☁)</li>
  <li><strong>No more internet dependency</strong></li>
  <li><strong>No usage costs</strong></li>
  <li><strong>Full control over model behavior</strong></li>
</ul>

<h2 id="quick-setup">Quick Setup</h2>
<p>I made this guide to be as simple as possible (for you beginners), so we’ll use Ollama, a simple CLI tool for using local AIs.</p>
<h3 id="install-ollama">Install Ollama</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Mac</span>
curl https://ollama.ai/install.sh | sh

<span class="c"># Linux</span>
curl <span class="nt">-fsSL</span> https://ollama.ai/install.sh | sh
</code></pre></div></div>
<p>For Windows, go to https://ollama.com/download/windows and follow the instructions.</p>
<h3 id="basic-usage">Basic Usage</h3>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Pull a model</span>
ollama pull llama2

<span class="c"># Basic interaction</span>
ollama run llama2
<span class="c"># or, if you want to use a specific parameter size model</span>
ollama run llama2:13b
</code></pre></div></div>
<p>Honestly, skipping the pull command is possible, but it needs an extra step (which is downloading) before it can run the model.</p>

<h3 id="api-integration">API Integration</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Simple API call</span>
curl <span class="nt">-X</span> POST http://localhost:11434/api/generate <span class="nt">-d</span> <span class="s1">'{
    "model": "llama2",
    "prompt": "Why is local AI important?"
}'</span>
</code></pre></div></div>

<h2 id="some-of-my-favorite-models">(some of my) Favorite Models!</h2>
<ul>
  <li><strong>llama2</strong>: good all-rounder</li>
  <li><strong>codellama</strong>: programming focused</li>
  <li><strong>mistral</strong>: great performance/size ratio
If you want to go ahead and run some of the best AI models:</li>
  <li><strong>gpt4all</strong>: tiny, experimental, great for offline demos (very low resource)</li>
  <li><strong>vicuna</strong>: chat-focused, tuned for conversational assistants (mid-weight)</li>
  <li><strong>falcon (7B variant)</strong>: high-quality, efficient 7B model (performance-oriented)</li>
</ul>

<p>If you also have higher resources (more RAM / GPUs), consider larger variants:</p>
<ul>
  <li><strong>llama2 (13B / 70B variants)</strong>: scalable chat and instruction tuning; pick 13B for a balance, 70B for top quality</li>
  <li><strong>falcon (40b variant)</strong>: strong 40B option for high-quality generation (heavy)</li>
  <li><strong>Mistral / other large instruction-tuned models (e.g., large Mistral variants)</strong>: great tradeoffs depending on release/version. use <code class="language-plaintext highlighter-rouge">mistral:large</code></li>
</ul>

<h2 id="performance-notes">Performance Notes</h2>
<ul>
  <li>RAM usage varies by model</li>
  <li>16GB RAM minimum recommended</li>
  <li>GPU acceleration helps significantly</li>
</ul>

<h2 id="where-would-this-be-used">Where would this be used?</h2>
<p>Well, it would be useful for cutting down costs significantly, and also for more privacy. Big companies take your chats and (probably) sell them. You don’t want that, do you? Also, you don’t need to invest in much money for AI models like GPT-5 (if you use it more than usual).</p>

<h2 id="resources-">Resources :)</h2>
<ul>
  <li><a href="https://github.com/ollama/ollama">Ollama GitHub</a></li>
  <li><a href="https://ollama.ai/library">Model Library</a></li>
</ul>

<h2 id="personal-tips">Personal Tips</h2>
<ul>
  <li>Keep smaller models for quick tasks</li>
  <li>Use modelfiles for customization</li>
  <li>Watch RAM usage with larger models</li>
</ul>

<p class="note">These are notes from my experience with local AI models. Your mileage may vary based on hardware and specific use cases.</p>]]></content><author><name>kev</name><email>kevinm.s@outlook.co.id</email></author><category term="Artificial Intelligence" /><category term="Local Development" /><summary type="html"><![CDATA[Local AI models are revolutionizing how we interact with artificial intelligence. This devnote explores Ollama, a powerful tool for running AI models locally on your own PC. I’ll cover everything you need to know about getting started with local AI development.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/img/ollama.png" /><media:content medium="image" url="http://localhost:4000/assets/img/ollama.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>